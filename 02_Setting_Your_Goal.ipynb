{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single number evaluation metric\n",
    "\n",
    "- **Set up a real number evaluation metrics for your problem**\n",
    "- We cannot use precision/recall because we understanding/analyzing performance across different models using two metrics is not efficient. We cannot be sure of how well our models is performing against one another.\n",
    "- We should **f1-score** because we have one well define evaluation metric to compare across different models.\n",
    "- Another method is to use the average across multiple classification possiblities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satisficing and Optimizing metric\n",
    "\n",
    "| Classifer | Accuracy | Running Time |\n",
    "| --- | --- | --- |\n",
    "| A | 90% | 80ms |\n",
    "| B | 92% | 95ms |\n",
    "| C | 95% | 1500ms |\n",
    "\n",
    "Another way to compute performance is across a performance metrics and a computational metrics (the practicality of running certain models)\n",
    "\n",
    "One method:\n",
    "- Cost = Accuracy - 0.5(Running Time)\n",
    "\n",
    "Another method:\n",
    "- Maximize: accuracy subject to running time to be less than or equal to 100MS\n",
    "    - Optimizing metric: Accuracy\n",
    "    - Satisfying metric: Running Time\n",
    "    \n",
    "Real world Exampe: Triggers words\n",
    "- Trigger words are words that \"wake up\" Alexa or Siri. \n",
    "- Hence, a good cost equation we could use is:\n",
    "    - Maximize accuracy subject to less than or equal to 1 false positive per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/dev/test distributions\n",
    "\n",
    "Cat classification dev/test sets\n",
    "- dev: development set, hold out cross-validation sets\n",
    "\n",
    "We should not use differerent dataset distributions for the test and development set. \n",
    "- For example, if regions is a part of our datasets, we should use different/separate data structure (like certain regions for test set while another region is for development set)\n",
    "\n",
    "True Story:\n",
    "- Development stage: For loan approvals, Andrew discuss how the use middle-class zip codes on a home loan approval problem.\n",
    "- Testing stage: However, they tested the model on low-income zip codes.\n",
    "- The distribution between middle-class and low-income zip codes is very differernt so it did not work well on low income zip codes.\n",
    "\n",
    "Guideline:\n",
    "- Choose a development set and test set to reflect data you expect to get in the future and consider important to do well on.\n",
    "    - Development and test set should be the same distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of the dev and test sets\n",
    "\n",
    "Old way of splitting: (100, 1000, or 10000 examples)\n",
    "- 70 percent train, while 30 percent would be test\n",
    "- 60 percent train, 20 percent dev, 20 percent test\n",
    "\n",
    "New way of splitting (1000000 examples)\n",
    "- 98 percent train, 1 percent dev, and 1 percent train\n",
    "\n",
    "Size of test set:\n",
    "- Set your test to be big enough to give high confidence in the overall performance of your system\n",
    "\n",
    "Sometimes you will not even need a test set (only a train and dev set)\n",
    "- Not usually recommened though\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to change dev/test sets and metrics\n",
    "\n",
    "\n",
    "| Classification Error | Model A: | Model B |\n",
    "| --- | --- | --- |\n",
    "| --- | 3% error | 5% error |\n",
    "\n",
    "\n",
    "When comparing performance metics, numbers will not always fully capture the full picture. \n",
    "- For example, in the table above, Model A has a lower classification error. But, the error term cannot explain what is causing the error. So while Model A has a lower classification error, it's always not capturing *ALL* pornographic images as an error. Hence, Model B has a greater classification error without pornographic images.\n",
    "\n",
    "\n",
    "**Error Formula**\n",
    "- $1/M_{dev}\\sum^{M_{dev}}_{i=1}I(y_{pred}^{i} + y^{i})$\n",
    "- Predicted value is 0 or 1\n",
    "- Moreover, the equation provides the total number of 1s\n",
    "- Treats pornographic and non-pornographic image equally\n",
    "\n",
    "**Updated Error**\n",
    "- $1/M_{dev}\\sum^{M_{dev}}_{i=1}w^{(i)}I(y_{pred}^{i} + y^{i})$**\n",
    "- $w^{(1)}$ = 1 if $x^{(i)}$ is non-porn, 10 if $x^{(i)}$ is a pornographic image\n",
    "- We are penalizing the error function with a greater weight if there are pornographic imaged\n",
    "\n",
    "Orthogonalization for cat pictures: anti-porn\n",
    "- Must be broken down into two pieces\n",
    "    1. Figure out the performance metric you want the model to be evaluated on\n",
    "    2. Work on improving the performance metric\n",
    "\n",
    "However, the paradox that a lower performance metric does not signify a better training model could result from...\n",
    "- If the actual data does not reflect our development/testing set. If we are predicting images, and the images that people use are not high quality, then we should consider in using a different dataset that most resembles the actual data.\n",
    "\n",
    "**Lastly, do not take much time in choosing the performance metic**\n",
    "- Make sure you have a performance metric and train your model to optimize for it. If later down the line, you realize that the performance metric is not optimal, you can always change it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
