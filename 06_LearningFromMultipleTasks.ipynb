{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from multiple tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "With transfer learning, the idea is to use previous models on other datasets. For example, if we are dealing with image recognition, we might later want to learn how to build a model that is able to test on radiology images and their diagnosisis. \n",
    "\n",
    "<img src=\"./images/struct_07.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "One method:\n",
    "- You can randomize the weights and biases of the last layer. Thus, during the first part of the layers, the learning is the same. However, since we are changing the weights for the last layers, then our hope is that we could find new learning.\n",
    "\n",
    "Second method: \n",
    "- If there's enough data, we could re-train the entire layers.\n",
    "    - **Pre-Trained**: In our example, this would be the image recognition component.\n",
    "    - Since we are using the pre-trained data, to initiliazed the weights\n",
    "    - **Fine-Tuning**: Here, we are using the radiology dataset to update the weights\n",
    "\n",
    "<img src=\"./images/struct_08.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Other examples of transfer learning could be speech recognition:\n",
    "- For example, we could have a speech recognition NN and we would like to use it for a wakeword/trigger problem\n",
    "- *Remember: A wakeword/trigger problem is one where the devices wake up with a war (ie Alexa)*\n",
    "- We could remove the last layers and not only add one layer but add multiples layers if we wanted to\n",
    "\n",
    "When should you use transfer learning?\n",
    "- If we have a lot of data in our **pre-trained** datasets but not as much in the **fine-tuning** datasets\n",
    "- Task A and B have the same input\n",
    "- Low level features from A could be helpful for learning B\n",
    "    - In our examples, the image recognition could be helpful in reading images. Or the speech recognition could be helpful in learning human's voices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task learning\n",
    "\n",
    "The goal of multi-task learning is to train the model to multi-tasks (and they are not task that are mutually exclusive) i.e. we could train an autonomous car to learn if they are pedestrians, cars, stop signs, and traffic lights...\n",
    "- Thus we have a four by 1 vector for y \n",
    "\n",
    "<img src=\"./images/struct_03.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"./images/struct_03.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "- With this new cost function, we are calculating the miscalculations for all the tasks we are training for.\n",
    "    \n",
    "We are solving four problems. We could have trained four neural networks. However, if a lot of the core is the same for all the tasks, the neural networks are best when they use the same core.\n",
    "- This also works great if some of the task were not labeled correctly. You could still train it to learn four task at once!\n",
    "\n",
    "When does multi-tasking learning make sense?\n",
    "- Training on a set of tasks that could beneift from having shared lower-level features\n",
    "- Usually: Amount of data you have for each task is quite similar.\n",
    "- Can train a big enough neural network to do well on all the tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
